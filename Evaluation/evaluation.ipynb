{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate LLM responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.spatial.distance import jensenshannon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compares files in ground-truth folder, and predictions to find missing files, utterances or incorrect decision output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folders(predictions_folder, ground_truth_folder):\n",
    "    pred_files = {os.path.basename(f) for f in glob.glob(os.path.join(predictions_folder, '*.csv'))}\n",
    "    gt_files = {os.path.basename(f) for f in glob.glob(os.path.join(ground_truth_folder, '*.csv'))}\n",
    "\n",
    "    logging.info(f\"Number of files in predictions folder: {len(pred_files)}\")\n",
    "    logging.info(f\"Number of files in ground truth folder: {len(gt_files)}\")\n",
    "\n",
    "    missing_in_pred = gt_files - pred_files\n",
    "    missing_in_gt = pred_files - gt_files\n",
    "\n",
    "    if missing_in_pred:\n",
    "        logging.warning(f\"Files missing in predictions: {missing_in_pred}\")\n",
    "    if missing_in_gt:\n",
    "        logging.warning(f\"Files missing in ground truth: {missing_in_gt}\")\n",
    "\n",
    "    common_files = pred_files.intersection(gt_files)\n",
    "\n",
    "    for file in common_files:\n",
    "        predictions_file_path = os.path.join(predictions_folder, file)\n",
    "        ground_truth_file_path = os.path.join(ground_truth_folder, file)\n",
    "        \n",
    "        predictions_df = pd.read_csv(predictions_file_path)\n",
    "        ground_truth_df = pd.read_csv(ground_truth_file_path)\n",
    "\n",
    "        ground_truth_df = ground_truth_df[ground_truth_df['Utterance'].str.contains(\"^\\\\d+\\\\. Bot:\", regex=True)]\n",
    "\n",
    "        if len(predictions_df) != len(ground_truth_df):\n",
    "            logging.warning(f\"Mismatch in the number of bot responses in file: {file}. Predictions have {len(predictions_df)}, Ground Truth has {len(ground_truth_df)}\")\n",
    "            \n",
    "        if not all(predictions_df['decision'].str.lower().isin(['breakdown', 'non-breakdown'])):\n",
    "            logging.warning(f\"Invalid decision in file: {file}.\")\n",
    "    return len(pred_files), len(gt_files), missing_in_pred, missing_in_gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_to_binary(decision):\n",
    "    \"\"\"\n",
    "    Converts decision text to binary.\n",
    "    If the decision contains 'non-breakdown', it returns 0.\n",
    "    Otherwise, it returns 1.\n",
    "    \"\"\"\n",
    "    if 'non-breakdown' in decision.lower():\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used to check individual files to verify if functions are all good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(predictions_file, ground_truth_file):\n",
    "    predictions_df = pd.read_csv(predictions_file)\n",
    "    ground_truth_df = pd.read_csv(ground_truth_file)\n",
    "\n",
    "    # preprocess df to only include Bot utterances for ground truth\n",
    "    ground_truth_df = ground_truth_df[ground_truth_df['Utterance'].str.contains(\"^\\\\d+\\\\. Bot:\", regex=True)]\n",
    "    # extract line numbers to align both DataFrames\n",
    "    ground_truth_df['Line Number'] = ground_truth_df['Utterance'].apply(lambda x: int(x.split('.')[0]))\n",
    "    predictions_df['Line Number'] = predictions_df['segment'].apply(lambda x: int(x.split('.')[0]))\n",
    "\n",
    "    # merge on Line Number\n",
    "    merged_df = pd.merge(predictions_df, ground_truth_df, on='Line Number', how='left')\n",
    "\n",
    "    merged_df['Prediction'] = merged_df['decision'].apply(decision_to_binary)\n",
    "    merged_df['Truth'] = merged_df['Majority Voting'].map({'non-breakdown': 0, 'breakdown': 1})\n",
    "    merged_df = merged_df.dropna(subset=['Majority Voting']) # used to drop the first bot response in some cases (NaN)\n",
    "    merged_df['Truth'] = merged_df['Truth'].astype(int)\n",
    "\n",
    "    accuracy = accuracy_score(merged_df['Truth'], merged_df['Prediction'])\n",
    "    recall = recall_score(merged_df['Truth'], merged_df['Prediction'])\n",
    "    precision = precision_score(merged_df['Truth'], merged_df['Prediction'])\n",
    "    f1 = f1_score(merged_df['Truth'], merged_df['Prediction'])\n",
    "    mse = mean_squared_error(merged_df['Truth'], merged_df['Prediction'])\n",
    "\n",
    "    # For JS Divergence, convert binary classifications to a basic probability distribution for each case\n",
    "    predictions_prob = merged_df['Prediction'].value_counts(normalize=True).reindex([0, 1]).fillna(0)\n",
    "    ground_truth_prob = merged_df['Truth'].value_counts(normalize=True).reindex([0, 1]).fillna(0)\n",
    "    js_divergence = (jensenshannon(predictions_prob, ground_truth_prob)**2)\n",
    "\n",
    "    return accuracy, recall, precision, f1, js_divergence, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_folder(predictions_folder, ground_truth_folder):\n",
    "    predictions_files = glob.glob(os.path.join(predictions_folder, '*.csv'))\n",
    "    ground_truth_files = glob.glob(os.path.join(ground_truth_folder, '*.csv'))\n",
    "\n",
    "    predictions_set = set([os.path.basename(f) for f in predictions_files])\n",
    "    ground_truth_set = set([os.path.basename(f) for f in ground_truth_files])\n",
    "\n",
    "    common_files = predictions_set.intersection(ground_truth_set)\n",
    "\n",
    "    total_accuracy, total_recall, total_precision, total_f1, total_js_divergence, total_mse = [], [], [], [], [], []\n",
    "\n",
    "    total_squared_error = 0.0\n",
    "    total_count = 0\n",
    "    total_correct = 0\n",
    " \n",
    "\n",
    "    for file in common_files:\n",
    "        predictions_file_path = os.path.join(predictions_folder, file)\n",
    "        ground_truth_file_path = os.path.join(ground_truth_folder, file)\n",
    "        \n",
    "        accuracy, recall, precision, f1, js_divergence, mse = evaluate_predictions(predictions_file_path, ground_truth_file_path)\n",
    "        \n",
    "        total_accuracy.append(accuracy)\n",
    "        total_recall.append(recall)\n",
    "        total_precision.append(precision)\n",
    "        total_f1.append(f1)\n",
    "        total_js_divergence.append(js_divergence)\n",
    "        total_mse.append(mse)\n",
    "\n",
    "        predictions_df = pd.read_csv(predictions_file_path)\n",
    "        ground_truth_df = pd.read_csv(ground_truth_file_path)\n",
    "\n",
    "        ground_truth_df = ground_truth_df[ground_truth_df['Utterance'].str.contains(\"^\\\\d+\\\\. Bot:\", regex=True)]\n",
    "\n",
    "        ground_truth_df['Line Number'] = ground_truth_df['Utterance'].apply(lambda x: int(x.split('.')[0]))\n",
    "        predictions_df['Line Number'] = predictions_df['segment'].apply(lambda x: int(x.split('.')[0]))\n",
    "\n",
    "        merged_df = pd.merge(predictions_df, ground_truth_df, on='Line Number', how='left')\n",
    "\n",
    "        merged_df['Prediction'] = merged_df['decision'].apply(decision_to_binary)\n",
    "        merged_df['Truth'] = merged_df['Majority Voting'].map({'non-breakdown': 0, 'breakdown': 1})\n",
    "        merged_df = merged_df.dropna(subset=['Majority Voting'])\n",
    "        merged_df['Truth'] = merged_df['Truth'].astype(int)\n",
    "\n",
    "        correct = (merged_df['Prediction'] == merged_df['Truth']).sum()\n",
    "        squared_errors = merged_df.apply(lambda row: (row['Prediction'] - row['Truth']) ** 2, axis=1)\n",
    "\n",
    "        total_squared_error += squared_errors.sum()\n",
    "        total_count += len(squared_errors)\n",
    "        total_correct += correct\n",
    "\n",
    "    avg_accuracy = sum(total_accuracy) / len(total_accuracy)\n",
    "    avg_recall = sum(total_recall) / len(total_recall)\n",
    "    avg_precision = sum(total_precision) / len(total_precision)\n",
    "    avg_f1 = sum(total_f1) / len(total_f1)\n",
    "    avg_js_divergence = sum(total_js_divergence) / len(total_js_divergence)\n",
    "    avg_mse = sum(total_mse) / len(total_mse)\n",
    "    combined_mse = total_squared_error / total_count if total_count > 0 else float('nan')\n",
    "\n",
    "    print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "    print(f\"Average JS Divergence: {avg_js_divergence:.4f}\")\n",
    "    print(f\"Average MSE: {avg_mse:.4f}\")\n",
    "    print(f\"Combined MSE: {combined_mse:.4f}\")\n",
    "\n",
    "    #return total_accuracy, total_recall, total_precision, total_f1, total_js_divergence, total_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/two_shot/gpt3'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/two_shot/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/CoT/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/zero_shot/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/two_shot_CoT/gpt3'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/two_shot_CoT/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/zero_shot_v1/gpt3'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/zero_shot/gpt3'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/zero_shot_v1/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/zero_shot/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step-by-step prompt:\n",
    "\n",
    "    Your analysis should include:\n",
    "    - A step-by-step reasoning process explaining how you reached your conclusion.\n",
    "    - A final decision categorized as either 'BREAKDOWN' or 'NON-BREAKDOWN'.\n",
    "    - A score from 0 to 1, where 0 represents a total breakdown and 1 indicates a completely smooth conversation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/CoT_v1/gpt3'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/CoT_v1/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Guided Step-By-Step Prompt to rationalize the thinking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/CoT_long/gpt3'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/CoT_long/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final CoT Prompt:\n",
    "\n",
    "            - Before answering, you should think through the question step-by-step.\n",
    "            - Explain your reasoning at each step towards making your final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/CoT/gpt3'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/CoT/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/CoT_long/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/CoT_v1/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/CoT_v2/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/CoT/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Shot - BREAKDOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/one_shot_b/gpt3'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/one_shot/gpt3'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/one_shot/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/two_shot/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/two_shot_v1/gpt3'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/two_shot_v2/gpt3'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/two_shot_v2/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/two_shot_v3/gpt3'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Shot - v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/two_shot/gpt3'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/two_shot/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Shot CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/two_shot_CoT/gpt3'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/two_shot_CoT/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/four_shot/gpt3'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/four_shot/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/four_shot_CoT/gpt3'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/four_shot_CoT/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/zero_shot/medium'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/CoT/medium'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/four_shot/medium'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/two_shot/medium'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/two_shot_CoT/medium'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/four_shot_CoT/medium'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/CoT/medium'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/anal_reason/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = 'results/CoT/gpt4'\n",
    "ground_truth_folder = 'db/eval_labelled'\n",
    "check_folders(predictions_folder, ground_truth_folder)\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_folder = '../Results/CoT/gpt3' \n",
    "ground_truth_folder = '../db/eval_labelled'\n",
    "evaluate_folder(predictions_folder, ground_truth_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
